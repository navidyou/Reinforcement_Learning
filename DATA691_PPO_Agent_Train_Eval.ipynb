{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1VyeoMHeRw1"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gym-foo.zip\n",
        "!pip install -e /content/gym-foo"
      ],
      "metadata": {
        "id": "IQ-ZHFptejlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents"
      ],
      "metadata": {
        "id": "zO25tctIemuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "N293Yz1Weq0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "from absl import logging\n",
        "from tf_agents.environments.py_environment import PyEnvironment\n",
        "from tf_agents.environments.tf_environment import TFEnvironment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.trajectories.policy_step import PolicyStep\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import gin\n",
        "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
        "from tf_agents.agents.ppo import ppo_clip_agent\n",
        "from tf_agents.drivers import dynamic_episode_driver\n",
        "from tf_agents.environments import parallel_py_environment\n",
        "from tf_agents.environments import suite_mujoco\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.networks import actor_distribution_rnn_network\n",
        "from tf_agents.networks import value_network\n",
        "from tf_agents.networks import value_rnn_network\n",
        "from tf_agents.policies import policy_saver\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.system import system_multiprocessing as multiprocessing\n",
        "from tf_agents.utils import common\n",
        "\n",
        "import gym_foo\n",
        "from tf_agents.environments import suite_gym"
      ],
      "metadata": {
        "id": "tizPw5gke2ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_video(py_environment: PyEnvironment, tf_environment: TFEnvironment, policy, num_episodes=1, video_filename='imageio.mp4'):\n",
        "  logging.info(\"Generating video %s\" % video_filename)\n",
        "\t#with imageio.get_writer(video_filename, fps=60) as video:\n",
        "  with imageio.get_writer(video_filename, fps=1) as video:\n",
        "    for episode in range(num_episodes):\n",
        "      logging.info(\"Generating episode %d of %d\" % (episode, num_episodes))\n",
        "\n",
        "      time_step = tf_environment.reset()\n",
        "      state = policy.get_initial_state(tf_environment.batch_size)\n",
        "\n",
        "      video.append_data(py_environment.render())\n",
        "\n",
        "      while not time_step.is_last():\n",
        "        policy_step: PolicyStep = policy.action(time_step, state)\n",
        "        #state = policy_step.state\n",
        "        #act = policy_step.action\n",
        "\n",
        "        time_step = tf_environment.step(policy_step.action)\n",
        "        #time_step = tf_environment.step(act)\n",
        "        #time_step = tf_environment.step(policy_step.action)\n",
        "        img_bgr = py_environment.render()\n",
        "        video.append_data(img_bgr)\n",
        "\n",
        "  logging.info(\"Finished video %s\" % video_filename)"
      ],
      "metadata": {
        "id": "Zb7Wsz_Ie8CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def env_load_fn(env_name):\n",
        "\n",
        "  py_env = suite_gym.load(env_name)\n",
        "\n",
        "  py_env.reset()\n",
        "\n",
        "  return py_env"
      ],
      "metadata": {
        "id": "Yy0SWf2efE_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "flags.DEFINE_string('root_dir', '/content/gdrive/My Drive/DATA691_RL_logs10',\n",
        "                    'Root directory for writing logs/summaries/checkpoints.')\n",
        "flags.DEFINE_string('env_name', 'foo-v0', 'Name of an environment')\n",
        "flags.DEFINE_integer(\n",
        "    'replay_buffer_capacity', 1001, 'Replay buffer capacity per env.'\n",
        ")\n",
        "flags.DEFINE_integer(\n",
        "    #'num_parallel_environments', 16, 'Number of environments to run in parallel'\n",
        "    'num_parallel_environments', 256, 'Number of environments to run in parallel'\n",
        ")\n",
        "flags.DEFINE_integer(\n",
        "    'num_environment_steps',\n",
        "    25000000,\n",
        "    'Number of environment steps to run before finishing.',\n",
        ")\n",
        "flags.DEFINE_integer(\n",
        "    'num_epochs', 25, 'Number of epochs for computing policy updates.'\n",
        ")\n",
        "flags.DEFINE_integer(\n",
        "    'collect_episodes_per_iteration',\n",
        "    30,\n",
        "    'The number of episodes to take in the environment before '\n",
        "    'each update. This is the total across all parallel '\n",
        "    'environments.',\n",
        ")\n",
        "flags.DEFINE_integer(\n",
        "    'num_eval_episodes', 30, 'The number of episodes to run eval on.'\n",
        ")\n",
        "flags.DEFINE_boolean(\n",
        "    #'use_rnns', False, 'If true, use RNN for policy and value function.'\n",
        "    'use_rnns', True, 'If true, use RNN for policy and value function.'\n",
        ")\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "@gin.configurable\n",
        "def train_eval(\n",
        "    root_dir,\n",
        "    #env_name='HalfCheetah-v2',\n",
        "    env_name='foo-v0',\n",
        "    #env_load_fn=suite_mujoco.load,\n",
        "    env_load_fn=env_load_fn,\n",
        "    random_seed=None,\n",
        "    # TODO(b/127576522): rename to policy_fc_layers.\n",
        "    actor_fc_layers=(200, 100),\n",
        "    value_fc_layers=(200, 100),\n",
        "    #actor_fc_layers=(400, 200),\n",
        "    #value_fc_layers=(400, 200),\n",
        "    use_rnns=False,\n",
        "    #lstm_size=(20,),\n",
        "    lstm_size=(64,),\n",
        "    # Params for collect\n",
        "    num_environment_steps=25000000,\n",
        "    collect_episodes_per_iteration=30,\n",
        "    num_parallel_environments=30,\n",
        "    replay_buffer_capacity=1001,  # Per-environment\n",
        "    # Params for train\n",
        "    num_epochs=25,\n",
        "    learning_rate=1e-3,\n",
        "    # Params for eval\n",
        "    num_eval_episodes=30,\n",
        "    eval_interval=100,\n",
        "    # Params for summaries and logging\n",
        "    train_checkpoint_interval=1000,\n",
        "    policy_checkpoint_interval=1000,\n",
        "    log_interval=100,\n",
        "    summary_interval=100,\n",
        "    summaries_flush_secs=1,\n",
        "    #use_tf_functions=True,\n",
        "    use_tf_functions=True,\n",
        "    debug_summaries=False,\n",
        "    summarize_grads_and_vars=False,\n",
        "):\n",
        "  \"\"\"A simple train and eval for PPO.\"\"\"\n",
        "  if root_dir is None:\n",
        "    raise AttributeError('train_eval requires a root_dir.')\n",
        "\n",
        "  root_dir = os.path.expanduser(root_dir)\n",
        "  train_dir = os.path.join(root_dir, 'train')\n",
        "  eval_dir = os.path.join(root_dir, 'eval')\n",
        "  saved_model_dir = os.path.join(root_dir, 'policy_saved_model')\n",
        "  videos_dir = os.path.join(root_dir, 'videos')\n",
        "  os.makedirs(videos_dir, exist_ok=True)\n",
        "\n",
        "  train_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
        "      train_dir, flush_millis=summaries_flush_secs * 1000\n",
        "  )\n",
        "  train_summary_writer.set_as_default()\n",
        "\n",
        "  eval_summary_writer = tf.compat.v2.summary.create_file_writer(\n",
        "      eval_dir, flush_millis=summaries_flush_secs * 1000\n",
        "  )\n",
        "  eval_metrics = [\n",
        "      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
        "      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes),\n",
        "  ]\n",
        "\n",
        "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "  with tf.compat.v2.summary.record_if(\n",
        "      lambda: tf.math.equal(global_step % summary_interval, 0)\n",
        "  ):\n",
        "    if random_seed is not None:\n",
        "      tf.compat.v1.set_random_seed(random_seed)\n",
        "    eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n",
        "    tf_env = tf_py_environment.TFPyEnvironment(\n",
        "        parallel_py_environment.ParallelPyEnvironment(\n",
        "            [lambda: env_load_fn(env_name)] * num_parallel_environments\n",
        "        )\n",
        "    )\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "    if use_rnns:\n",
        "      actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n",
        "          tf_env.observation_spec(),\n",
        "          tf_env.action_spec(),\n",
        "          input_fc_layer_params=actor_fc_layers,\n",
        "          output_fc_layer_params=None,\n",
        "          lstm_size=lstm_size,\n",
        "      )\n",
        "      value_net = value_rnn_network.ValueRnnNetwork(\n",
        "          tf_env.observation_spec(),\n",
        "          input_fc_layer_params=value_fc_layers,\n",
        "          output_fc_layer_params=None,\n",
        "      )\n",
        "    else:\n",
        "      actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "          tf_env.observation_spec(),\n",
        "          tf_env.action_spec(),\n",
        "          #(tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=4, output_mode=\"binary\"),\n",
        "           # tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=4, output_mode=\"binary\")),\n",
        "            #tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=2, output_mode=\"binary\")),\n",
        "          #tf.keras.layers.Concatenate(axis=-1),\n",
        "          fc_layer_params=actor_fc_layers,\n",
        "          activation_fn=tf.keras.activations.tanh,\n",
        "      )\n",
        "      value_net = value_network.ValueNetwork(\n",
        "          tf_env.observation_spec(),\n",
        "          #(tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=4, output_mode=\"binary\"),\n",
        "          #  tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=4, output_mode=\"binary\")),\n",
        "            #tf.keras.layers.experimental.preprocessing.CategoryEncoding(num_tokens=2, output_mode=\"binary\")),\n",
        "          #tf.keras.layers.Concatenate(axis=-1),\n",
        "          fc_layer_params=value_fc_layers,\n",
        "          activation_fn=tf.keras.activations.tanh,\n",
        "      )\n",
        "\n",
        "    tf_agent = ppo_clip_agent.PPOClipAgent(\n",
        "        tf_env.time_step_spec(),\n",
        "        tf_env.action_spec(),\n",
        "        optimizer,\n",
        "        actor_net=actor_net,\n",
        "        value_net=value_net,\n",
        "        entropy_regularization=0.001,\n",
        "        importance_ratio_clipping=0.7,\n",
        "        #entropy_regularization=0.0,\n",
        "        #importance_ratio_clipping=0.2,\n",
        "        normalize_observations=False,\n",
        "        normalize_rewards=False,\n",
        "        use_gae=True,\n",
        "        num_epochs=num_epochs,\n",
        "        debug_summaries=debug_summaries,\n",
        "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
        "        train_step_counter=global_step,\n",
        "    )\n",
        "    tf_agent.initialize()\n",
        "\n",
        "    environment_steps_metric = tf_metrics.EnvironmentSteps()\n",
        "    step_metrics = [\n",
        "        tf_metrics.NumberOfEpisodes(),\n",
        "        environment_steps_metric,\n",
        "    ]\n",
        "\n",
        "    train_metrics = step_metrics + [\n",
        "        tf_metrics.AverageReturnMetric(batch_size=num_parallel_environments),\n",
        "        tf_metrics.AverageEpisodeLengthMetric(\n",
        "            batch_size=num_parallel_environments\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    eval_policy = tf_agent.policy\n",
        "    collect_policy = tf_agent.collect_policy\n",
        "\n",
        "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "        tf_agent.collect_data_spec,\n",
        "        batch_size=num_parallel_environments,\n",
        "        max_length=replay_buffer_capacity,\n",
        "    )\n",
        "\n",
        "    train_checkpointer = common.Checkpointer(\n",
        "        ckpt_dir=train_dir,\n",
        "        agent=tf_agent,\n",
        "        global_step=global_step,\n",
        "        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'),\n",
        "    )\n",
        "    policy_checkpointer = common.Checkpointer(\n",
        "        ckpt_dir=os.path.join(train_dir, 'policy'),\n",
        "        policy=eval_policy,\n",
        "        global_step=global_step,\n",
        "    )\n",
        "    saved_model = policy_saver.PolicySaver(eval_policy, train_step=global_step)\n",
        "\n",
        "    train_checkpointer.initialize_or_restore()\n",
        "\n",
        "    collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "        tf_env,\n",
        "        collect_policy,\n",
        "        observers=[replay_buffer.add_batch] + train_metrics,\n",
        "        num_episodes=collect_episodes_per_iteration,\n",
        "    )\n",
        "\n",
        "    def train_step():\n",
        "      trajectories = replay_buffer.gather_all()\n",
        "      return tf_agent.train(experience=trajectories)\n",
        "\n",
        "    if use_tf_functions:\n",
        "      # TODO(b/123828980): Enable once the cause for slowdown was identified.\n",
        "      collect_driver.run = common.function(collect_driver.run, autograph=False)\n",
        "      tf_agent.train = common.function(tf_agent.train, autograph=False)\n",
        "      train_step = common.function(train_step)\n",
        "\n",
        "    collect_time = 0\n",
        "    train_time = 0\n",
        "    timed_at_step = global_step.numpy()\n",
        "\n",
        "    while environment_steps_metric.result() < num_environment_steps:\n",
        "      global_step_val = global_step.numpy()\n",
        "      if global_step_val % eval_interval == 0: #and global_step_val != 0:\n",
        "        metric_utils.eager_compute(\n",
        "            eval_metrics,\n",
        "            eval_tf_env,\n",
        "            eval_policy,\n",
        "            num_episodes=num_eval_episodes,\n",
        "            train_step=global_step,\n",
        "            summary_writer=eval_summary_writer,\n",
        "            summary_prefix='Metrics',\n",
        "        )\n",
        "\n",
        "      start_time = time.time()\n",
        "      collect_driver.run()\n",
        "      collect_time += time.time() - start_time\n",
        "\n",
        "      start_time = time.time()\n",
        "      total_loss, _ = train_step()\n",
        "      replay_buffer.clear()\n",
        "      train_time += time.time() - start_time\n",
        "\n",
        "      for train_metric in train_metrics:\n",
        "        train_metric.tf_summaries(\n",
        "            train_step=global_step, step_metrics=step_metrics\n",
        "        )\n",
        "\n",
        "      if global_step_val % log_interval == 0:\n",
        "        logging.info('step = %d, loss = %f', global_step_val, total_loss)\n",
        "        steps_per_sec = (global_step_val - timed_at_step) / (\n",
        "            collect_time + train_time\n",
        "        )\n",
        "        logging.info('%.3f steps/sec', steps_per_sec)\n",
        "        logging.info(\n",
        "            'collect_time = %.3f, train_time = %.3f', collect_time, train_time\n",
        "        )\n",
        "        with tf.compat.v2.summary.record_if(True):\n",
        "          tf.compat.v2.summary.scalar(\n",
        "              name='global_steps_per_sec', data=steps_per_sec, step=global_step\n",
        "          )\n",
        "\n",
        "        if global_step_val % train_checkpoint_interval == 0:\n",
        "          train_checkpointer.save(global_step=global_step_val)\n",
        "\n",
        "        if global_step_val % policy_checkpoint_interval == 0:\n",
        "\n",
        "          video_py_env = env_load_fn(env_name)\n",
        "          video_tf_env = tf_py_environment.TFPyEnvironment(video_py_env)\n",
        "          create_video(video_py_env, video_tf_env, tf_agent.policy, num_episodes=1, video_filename=os.path.join(videos_dir, \"video_%d.mp4\" % global_step_val))\n",
        "\n",
        "          policy_checkpointer.save(global_step=global_step_val)\n",
        "          saved_model_path = os.path.join(\n",
        "              saved_model_dir, 'policy_' + ('%d' % global_step_val).zfill(9)\n",
        "          )\n",
        "          saved_model.save(saved_model_path)\n",
        "\n",
        "        timed_at_step = global_step_val\n",
        "        collect_time = 0\n",
        "        train_time = 0\n",
        "\n",
        "    # One final eval before exiting.\n",
        "    metric_utils.eager_compute(\n",
        "        eval_metrics,\n",
        "        eval_tf_env,\n",
        "        eval_policy,\n",
        "        num_episodes=num_eval_episodes,\n",
        "        train_step=global_step,\n",
        "        summary_writer=eval_summary_writer,\n",
        "        summary_prefix='Metrics',\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "iDa61IsnethX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(_):\n",
        "    logging.set_verbosity(logging.INFO)\n",
        "    tf.compat.v1.enable_v2_behavior()\n",
        "    train_eval(\n",
        "      FLAGS.root_dir,\n",
        "      env_name=FLAGS.env_name,\n",
        "      use_rnns=FLAGS.use_rnns,\n",
        "      num_environment_steps=FLAGS.num_environment_steps,\n",
        "      collect_episodes_per_iteration=FLAGS.collect_episodes_per_iteration,\n",
        "      num_parallel_environments=FLAGS.num_parallel_environments,\n",
        "      replay_buffer_capacity=FLAGS.replay_buffer_capacity,\n",
        "      num_epochs=FLAGS.num_epochs,\n",
        "      num_eval_episodes=FLAGS.num_eval_episodes,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  flags.mark_flag_as_required('root_dir')\n",
        "  multiprocessing.handle_main(functools.partial(app.run, main))"
      ],
      "metadata": {
        "id": "hq8gGfgZfPrE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}